{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geoaigroup/geoaigroup-website/blob/main/content/media/SAM_26May2023/SAM_GEOAI_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1ae39ff"
      },
      "source": [
        "# Object masks from prompts with SAM\n",
        "This section was prepared by Ali Mayladan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4a4b25c"
      },
      "source": [
        "The Segment Anything Model (SAM) predicts object masks given prompts that indicate the desired object. The model first converts the image into an image embedding that allows high quality masks to be efficiently produced from a prompt.\n",
        "\n",
        "The `SamPredictor` class provides an easy interface to the model for prompting the model. It allows the user to first set an image using the `set_image` method, which calculates the necessary image embeddings. Then, prompts can be provided via the `predict` method to efficiently predict masks from those prompts. The model can take as input both point and box prompts, as well as masks from the previous iteration of prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "644532a8"
      },
      "source": [
        "## Environment Set-up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07fabfee"
      },
      "source": [
        "If running locally using jupyter, first install `segment_anything` in your environment using the [installation instructions](https://github.com/facebookresearch/segment-anything#installation) in the repository. If running from Google Colab, set `using_colab=True` below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/geoaigroup/geoaigroup-website/raw/main/content/media/SAM_26May2023/data.zip\n",
        "!unzip data.zip"
      ],
      "metadata": {
        "id": "RXi87ep3DNxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ea65efc"
      },
      "outputs": [],
      "source": [
        "using_colab = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91dd9a89"
      },
      "outputs": [],
      "source": [
        "if using_colab:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(\"PyTorch version:\", torch.__version__)\n",
        "    print(\"Torchvision version:\", torchvision.__version__)\n",
        "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "    import sys\n",
        "    !{sys.executable} -m pip install opencv-python matplotlib\n",
        "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "\n",
        "    # !mkdir images\n",
        "    # !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/truck.jpg\n",
        "    # !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/groceries.jpg\n",
        "\n",
        "    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0be845da"
      },
      "source": [
        "## Set-up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33681dd1"
      },
      "source": [
        "Necessary imports and helper functions for displaying points, boxes, and masks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSDcEYiPExdV"
      },
      "outputs": [],
      "source": [
        "!pip install geopandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69b28288"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import geopandas as gpd\n",
        "import os\n",
        "import json\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "from shapely.geometry import Point, Polygon\n",
        "import random\n",
        "from PIL import Image, ImageDraw\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29bc90d5"
      },
      "outputs": [],
      "source": [
        "def show_mask(mask,ax, random_color=False):\n",
        "\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "    #return mask_image\n",
        "\n",
        "def show_mask_box(mask, ax, random_color=False):\n",
        "\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "def show_points(coords, labels, ax, marker_size=375):\n",
        "    pos_points = coords[labels==1]\n",
        "    neg_points = coords[labels==0]\n",
        "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "\n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98b228b8"
      },
      "source": [
        "## Selecting objects with SAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bb1927b"
      },
      "source": [
        "First, load the SAM model and predictor. Change the path below to point to the SAM checkpoint. Running on CUDA and using the default model are recommended for best results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e28150b"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "model_type = \"vit_h\"\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "\n",
        "predictor = SamPredictor(sam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c925e829"
      },
      "source": [
        "Process the image to produce an image embedding by calling `SamPredictor.set_image`. `SamPredictor` remembers this embedding and will use it for subsequent mask prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8fc7a46"
      },
      "source": [
        "To select the truck, choose a point on it. Points are input to the model in (x,y) format and come with labels 1 (foreground point) or 0 (background point). Multiple points can be input; here we use only one. The chosen point will be shown as a star on the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OE0T7HXuUa5p"
      },
      "outputs": [],
      "source": [
        "def calculateIoU(gtMask, predMask):\n",
        "        # Calculate the true positives,\n",
        "        # false positives, and false negatives\n",
        "        tp = 0\n",
        "        fp = 0\n",
        "        fn = 0\n",
        "\n",
        "        for i in range(len(gtMask)):\n",
        "            for j in range(len(gtMask[0])):\n",
        "                if gtMask[i][j] == 1 and predMask[i][j] == 1:\n",
        "                    tp += 1\n",
        "                elif gtMask[i][j] == 0 and predMask[i][j] == 1:\n",
        "                    fp += 1\n",
        "                elif gtMask[i][j] == 1 and predMask[i][j] == 0:\n",
        "                    fn += 1\n",
        "\n",
        "        # Calculate IoU\n",
        "        iou = tp / (tp + fp + fn)\n",
        "\n",
        "        return iou"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYMpKgMa2C_z"
      },
      "outputs": [],
      "source": [
        "width = 512\n",
        "height = 512\n",
        "\n",
        "def convert_polygon_to_mask(geo):\n",
        "      gtmask=np.zeros((512,512))\n",
        "      for orig_row in geo:\n",
        "            polygon=[]\n",
        "\n",
        "            if orig_row.geom_type==\"Polygon\":\n",
        "                for point in orig_row.exterior.coords:\n",
        "                    polygon.append(point)\n",
        "                img = Image.new('L', (width, height), 0)\n",
        "                ImageDraw.Draw(img).polygon(polygon, outline=1, fill=1)\n",
        "                gt_mask_building = np.array(img)\n",
        "                gtmask=gtmask+gt_mask_building\n",
        "            else:\n",
        "                for x in orig_row.geoms:\n",
        "                 for point in x.exterior.coords:\n",
        "                    polygon.append(point)\n",
        "\n",
        "                img = Image.new('L', (width, height), 0)\n",
        "                ImageDraw.Draw(img).polygon(polygon, outline=1, fill=1)\n",
        "                gt_mask_building = np.array(img)\n",
        "                gtmask=gtmask+gt_mask_building\n",
        "      return gtmask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c69570c"
      },
      "outputs": [],
      "source": [
        "shapefile=\"data/shapefile\"\n",
        "images='data/images'\n",
        "orig_shp=\"data/orig_shp\"\n",
        "output_dir=\"data/output_images\"\n",
        "score_dir=\"data/scores\"\n",
        "\n",
        "flag=0\n",
        "width = 512\n",
        "height = 512\n",
        "\n",
        "for image in tqdm(os.listdir(images)):\n",
        "\n",
        "    name=image.split('.')[0]\n",
        "\n",
        "    image = cv2.imread(images+'/'+image)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    predictor.set_image(image)\n",
        "\n",
        "    for i,orig_g in zip(glob.glob(shapefile+'/'+name),glob.glob(orig_shp+'/'+name)):\n",
        "        input_point=[]\n",
        "        input_label=[]\n",
        "        mask_tile=np.zeros((3,512,512))\n",
        "        mask_tile0=np.zeros((512,512))\n",
        "        mask_tile1=np.zeros((512,512))\n",
        "        mask_tile2=np.zeros((512,512))\n",
        "        score_tile_buil=[]\n",
        "        score_tile=[]\n",
        "        gt_m=np.zeros((512,512))\n",
        "        try:\n",
        "          f=gpd.read_file(i)\n",
        "          geo=f['geometry']\n",
        "          f1=gpd.read_file(orig_g)\n",
        "          geo1=f1['geometry']\n",
        "          object_id=f['OBJECTID']\n",
        "          gt_m=convert_polygon_to_mask(geo1)\n",
        "\n",
        "        except Exception as e :\n",
        "          print(e)\n",
        "          flag=1\n",
        "          break\n",
        "        count=0\n",
        "\n",
        "        for row,oid in zip(geo,object_id):\n",
        "\n",
        "            x=row.x\n",
        "            y=row.y\n",
        "            i=[x,y]\n",
        "\n",
        "\n",
        "            input_point.append(i)\n",
        "            input_label.append(1)\n",
        "            i=np.array([i])\n",
        "            lab=np.array([1])\n",
        "\n",
        "            masks, scores, logits = predictor.predict(\n",
        "            point_coords=i,\n",
        "            point_labels=lab,\n",
        "            multimask_output=True,\n",
        "            )\n",
        "            masks=np.array(masks)\n",
        "            mask_tile=mask_tile+masks\n",
        "\n",
        "\n",
        "            msk0=masks[0]\n",
        "            mask_tile0=mask_tile0+msk0\n",
        "            msk1=masks[1]\n",
        "            mask_tile1=mask_tile1+msk1\n",
        "            msk2=masks[2]\n",
        "            mask_tile2=mask_tile2+msk2\n",
        "\n",
        "        iou0=calculateIoU(gt_m,mask_tile0)\n",
        "        iou1=calculateIoU(gt_m,mask_tile1)\n",
        "        iou2=calculateIoU(gt_m,mask_tile2)\n",
        "\n",
        "\n",
        "        scores=[iou0,iou1,iou2]\n",
        "        if flag==1:\n",
        "          flag=0\n",
        "          continue\n",
        "        input_point=np.array(input_point)\n",
        "        input_label=np.array(input_label)\n",
        "\n",
        "        for i,(mask,score) in enumerate(zip(mask_tile,scores)):\n",
        "            plt.figure(figsize=(10,10))\n",
        "            plt.imshow(image)\n",
        "            show_mask(mask, plt.gca())\n",
        "            show_points(input_point, input_label, plt.gca())\n",
        "            plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45ddbca3"
      },
      "source": [
        "## Batched boxes inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df6f18a0"
      },
      "source": [
        "SamPredictor can take multiple input prompts for the same image, using `predict_torch` method. This method assumes input points are already torch tensors and have already been transformed to the input frame. For example, imagine we have several box outputs from an object detector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6543ab76"
      },
      "outputs": [],
      "source": [
        "shapefile=\"data/orig_shp\"\n",
        "images='data/images'\n",
        "output_dir=\"data/output_images\"\n",
        "score_dir=\"data/scores\"\n",
        "gt=\"data/gt\"\n",
        "\n",
        "score_val={}\n",
        "score_=[]\n",
        "score_mean=[]\n",
        "for image in tqdm(os.listdir(images)):\n",
        "    name=image.split('.')[0]\n",
        "    image = cv2.imread(images+'/'+image)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    os.makedirs(score_dir,exist_ok=True)\n",
        "    os.makedirs(output_dir+\"/\"+f'{name}',exist_ok=True)\n",
        "    predictor.set_image(image)\n",
        "  ######\n",
        "    for i in glob.glob(shapefile+'/'+name):\n",
        "        mask_tile=np.zeros((512,512))\n",
        "\n",
        "        tile_boxes=[]\n",
        "        f=gpd.read_file(i)\n",
        "        geo=f['geometry']\n",
        "\n",
        "        for p in geo:\n",
        "            inbox=[]\n",
        "            poly=p\n",
        "            xmin,ymin,xmax,ymax=poly.bounds\n",
        "\n",
        "            inbox=[xmin,ymin,xmax,ymax]\n",
        "\n",
        "            tile_boxes.append(inbox)\n",
        "\n",
        "\n",
        "        input_boxes=torch.tensor(tile_boxes, device=predictor.device)\n",
        "\n",
        "        transformed_boxes = predictor.transform.apply_boxes_torch(input_boxes, image.shape[:2])\n",
        "        masks, _, _ = predictor.predict_torch(\n",
        "            point_coords=None,\n",
        "            point_labels=None,\n",
        "            boxes=transformed_boxes,\n",
        "            multimask_output=False,\n",
        "        )\n",
        "\n",
        " #####calculating scores######\n",
        "        gtmask=convert_polygon_to_mask(geo)\n",
        "        #print(gtmask.shape)\n",
        "\n",
        "\n",
        "        msk=masks.clone()\n",
        "        msk=msk.int()\n",
        "        msk=msk.cpu().numpy()\n",
        "\n",
        "\n",
        "        for i in range(msk.shape[0]):\n",
        "\n",
        "          batch=msk[i]\n",
        "\n",
        "          for b in range(batch.shape[0]):\n",
        "            mask_tile=mask_tile+batch[b]\n",
        "\n",
        "        #print(\"mask_tile\",mask_tile.shape)\n",
        "        iou=calculateIoU(gtmask,mask_tile)\n",
        "        score_val[\"name\"]=name\n",
        "        score_val[\"val\"]=iou\n",
        "        score_.append(score_val)\n",
        "        score_mean.append(iou)\n",
        " ######\n",
        "\n",
        "\n",
        "\n",
        "        for mask in masks:\n",
        "            show_mask_box(mask.cpu().numpy(),plt.gca(), random_color=True)\n",
        "\n",
        "\n",
        "        for box in input_boxes:\n",
        "            show_box(box.cpu().numpy(),plt.gca())\n",
        "\n",
        "        plt.axis('on')\n",
        "\n",
        "        plt.savefig(f'{output_dir}/{name}/{name}_batch_box.png')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "with open(score_dir+'/scores.json', 'w') as ff:\n",
        "        json.dump(score_, ff)\n",
        "scores_mean=np.array(score_mean)\n",
        "sa=\"average_score for all masks: \"+str(np.mean(scores_mean))\n",
        "with open(score_dir+'/all_box_scores.json', 'w') as ff:\n",
        "        json.dump(sa, ff)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangSAM : https://github.com/luca-medeiros/lang-segment-anything\n",
        "# GroundingDINO: https://github.com/IDEA-Research/GroundingDINO\n",
        "# Segment Anything : https://github.com/facebookresearch/segment-anything\n",
        "\n",
        "### Language Segment-Anything is an open-source project that combines the power of instance segmentation and text prompts to generate masks for specific objects in images. Built on the recently released Meta model, segment-anything, and the GroundingDINO detection model, it's an easy-to-use and effective tool for object detection and image segmentation.\n",
        "\n",
        "This section was prepared by Hasan Moughnieh."
      ],
      "metadata": {
        "id": "oow_J4wm_qBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/geoaigroup/geoaigroup-website/raw/main/content/media/SAM_26May2023/LangSAM.zip\n",
        "!unzip LangSAM.zip"
      ],
      "metadata": {
        "id": "QyCDYhkDEWCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install opencv-python matplotlib\n",
        "!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "zAtDoF_-_4uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ],
      "metadata": {
        "id": "4Bia-pl5_61v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "%cd {HOME}/GroundingDINO\n",
        "!pip install -q -e .\n",
        "!pip install -q roboflow"
      ],
      "metadata": {
        "id": "0rYB8y40_8dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#useful functions for displaying results\n",
        "\n",
        "def load_ground_truth_masks(ground_truth_masks):\n",
        "    loaded_masks = []\n",
        "    mask_image = Image.open(ground_truth_masks).convert('L')\n",
        "    mask_array = np.array(mask_image)\n",
        "    return mask_array\n",
        "\n",
        "def compute_accuracy(predicted_masks, ground_truth_masks):\n",
        "    combined_predicted_mask = np.any(predicted_masks.numpy(), axis=0)\n",
        "    accuracy = np.sum(np.logical_and(combined_predicted_mask, ground_truth_masks)) / np.sum(np.logical_or(combined_predicted_mask, ground_truth_masks))\n",
        "    return accuracy\n",
        "\n",
        "def display_images_with_masks(image, masks):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "    ax1.axis('off')\n",
        "    ax1.imshow(image)\n",
        "    ax1.set_title('Original Image')\n",
        "\n",
        "    ax2.axis('off')\n",
        "    ax2.imshow(image)\n",
        "    ax2.set_title('Image with Masks')\n",
        "\n",
        "    num_masks = masks.shape[0]\n",
        "    for i in range(num_masks):\n",
        "        mask = masks[i].numpy()\n",
        "        mask = np.ma.masked_where(mask < 0.5, mask)\n",
        "        ax2.imshow(mask, alpha=0.5, cmap='jet')\n",
        "\n",
        "    accuracy = compute_accuracy(masks, ground_truth_masks)\n",
        "    # Adjust spacing between subplots\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Display the figure in the notebook\n",
        "    plt.show()\n",
        "    print(\"Accuracy:\",accuracy)"
      ],
      "metadata": {
        "id": "-wz3RS89_-AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hV0MCSJuLGc6"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "from torchvision.utils import draw_segmentation_masks\n",
        "\n",
        "MIN_AREA = 100\n",
        "\n",
        "\n",
        "def load_image(image_path: str):\n",
        "    return Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "\n",
        "def draw_image(image, masks, boxes, labels, alpha=0.4):\n",
        "    image = torch.from_numpy(image).permute(2, 0, 1)\n",
        "    image = draw_bounding_boxes(image, boxes, colors=['red'] * len(boxes), labels=labels, width=2)\n",
        "    image = draw_segmentation_masks(image, masks=masks, colors=['cyan'] * len(boxes), alpha=alpha)\n",
        "    return image.numpy().transpose(1, 2, 0)\n",
        "\n",
        "\n",
        "def get_contours(mask):\n",
        "    if len(mask.shape) > 2:\n",
        "        mask = np.squeeze(mask, 0) #make it a 3d array\n",
        "    mask = mask.astype(np.uint8) # [0,1] range\n",
        "    mask *= 255 # [0,255] range\n",
        "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    # findContours take 3 arguments :\n",
        "    #binary mask , contour retrieval mode(external contours),contour approximation method\n",
        "    #it returns two outputs , hierarchy and contours. hierachy is set as dummy variable \"_\"\n",
        "\n",
        "    effContours = []\n",
        "    for c in contours:\n",
        "        area = cv2.contourArea(c)\n",
        "        if area > MIN_AREA:\n",
        "            effContours.append(c)\n",
        "    return effContours\n",
        "\n",
        "\n",
        "def contour_to_points(contour):\n",
        "    pointsNum = len(contour)\n",
        "    contour = contour.reshape(pointsNum, -1).astype(np.float32)\n",
        "    points = [point.tolist() for point in contour]\n",
        "    return points\n",
        "\n",
        "\n",
        "def generate_labelme_json(binary_masks, labels, image_size, image_path=None):\n",
        "    \"\"\"Generate a LabelMe format JSON file from binary mask tensor.\n",
        "    Args:\n",
        "        binary_masks: Binary mask tensor of shape [N, H, W].\n",
        "        labels: List of labels for each mask.\n",
        "        image_size: Tuple of (height, width) for the image size.\n",
        "        image_path: Path to the image file (optional).\n",
        "    Returns:\n",
        "        A dictionary representing the LabelMe JSON file.\n",
        "    \"\"\"\n",
        "    num_masks = binary_masks.shape[0]\n",
        "    binary_masks = binary_masks.numpy()\n",
        "\n",
        "    json_dict = {\n",
        "        \"version\": \"4.5.6\",\n",
        "        \"imageHeight\": image_size[0],\n",
        "        \"imageWidth\": image_size[1],\n",
        "        \"imagePath\": image_path,\n",
        "        \"flags\": {},\n",
        "        \"shapes\": [],\n",
        "        \"imageData\": None\n",
        "    }\n",
        "\n",
        "    # Loop through the masks and add them to the JSON dictionary\n",
        "    for i in range(num_masks):\n",
        "        mask = binary_masks[i]\n",
        "        label = labels[i]\n",
        "        effContours = get_contours(mask)\n",
        "\n",
        "        for effContour in effContours:\n",
        "            points = contour_to_points(effContour)\n",
        "            shape_dict = {\n",
        "                \"label\": label,\n",
        "                \"line_color\": None,\n",
        "                \"fill_color\": None,\n",
        "                \"points\": points,\n",
        "                \"shape_type\": \"polygon\"\n",
        "            }\n",
        "\n",
        "            json_dict[\"shapes\"].append(shape_dict)\n",
        "\n",
        "    return json_dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from urllib import request\n",
        "\n",
        "import groundingdino.datasets.transforms as T\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from groundingdino.models import build_model\n",
        "from groundingdino.util import box_ops\n",
        "from groundingdino.util.inference import predict\n",
        "from groundingdino.util.slconfig import SLConfig\n",
        "from groundingdino.util.utils import clean_state_dict\n",
        "from huggingface_hub import hf_hub_download\n",
        "from segment_anything import sam_model_registry\n",
        "from segment_anything import SamPredictor\n",
        "\n",
        "SAM_MODELS = {\n",
        "    \"vit_h\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\",\n",
        "    \"vit_l\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\",\n",
        "    \"vit_b\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\"\n",
        "}\n",
        "\n",
        "\n",
        "def load_model_hf(repo_id, filename, ckpt_config_filename, device='cpu'):\n",
        "    cache_config_file = hf_hub_download(repo_id=repo_id, filename=ckpt_config_filename)\n",
        "\n",
        "    args = SLConfig.fromfile(cache_config_file)\n",
        "    model = build_model(args)\n",
        "    args.device = device\n",
        "\n",
        "    cache_file = hf_hub_download(repo_id=repo_id, filename=filename)\n",
        "    checkpoint = torch.load(cache_file, map_location='cpu')\n",
        "    log = model.load_state_dict(clean_state_dict(checkpoint['model']), strict=False)\n",
        "    print(f\"Model loaded from {cache_file} \\n => {log}\")\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "def transform_image(image) -> torch.Tensor:\n",
        "    transform = T.Compose([\n",
        "        T.RandomResize([800], max_size=1333),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    image_transformed, _ = transform(image, None)\n",
        "    return image_transformed\n",
        "\n",
        "\n",
        "class LangSAM():\n",
        "\n",
        "    def __init__(self, sam_type=\"vit_h\"):\n",
        "        self.sam_type = sam_type\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.build_groundingdino()\n",
        "        self.build_sam(sam_type)\n",
        "\n",
        "    def build_sam(self, sam_type):\n",
        "        url = SAM_MODELS[sam_type]\n",
        "        sam_checkpoint = os.path.basename(url)\n",
        "        if not os.path.exists(sam_checkpoint):\n",
        "            request.urlretrieve(url, sam_checkpoint)\n",
        "        sam = sam_model_registry[sam_type](checkpoint=sam_checkpoint)\n",
        "        sam.to(device=self.device)\n",
        "        self.sam = SamPredictor(sam)\n",
        "\n",
        "    def build_groundingdino(self):\n",
        "        ckpt_repo_id = \"ShilongLiu/GroundingDINO\"\n",
        "        ckpt_filenmae = \"groundingdino_swinb_cogcoor.pth\"\n",
        "        ckpt_config_filename = \"GroundingDINO_SwinB.cfg.py\"\n",
        "        self.groundingdino = load_model_hf(ckpt_repo_id, ckpt_filenmae, ckpt_config_filename)\n",
        "\n",
        "    def predict_dino(self, image_pil, text_prompt, box_threshold, text_threshold):\n",
        "        image_trans = transform_image(image_pil)\n",
        "        boxes, logits, phrases = predict(model=self.groundingdino,\n",
        "                                         image=image_trans,\n",
        "                                         caption=text_prompt,\n",
        "                                         box_threshold=box_threshold,\n",
        "                                         text_threshold=text_threshold)\n",
        "        W, H = image_pil.size\n",
        "        boxes = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
        "\n",
        "        return boxes, logits, phrases\n",
        "\n",
        "    def predict_sam(self, image_pil, boxes):\n",
        "        image_array = np.asarray(image_pil)\n",
        "        self.sam.set_image(image_array)\n",
        "        transformed_boxes = self.sam.transform.apply_boxes_torch(boxes, image_array.shape[:2])\n",
        "        masks, _, _ = self.sam.predict_torch(\n",
        "            point_coords=None,\n",
        "            point_labels=None,\n",
        "            boxes=transformed_boxes.to(self.sam.device),\n",
        "            multimask_output=False,\n",
        "        )\n",
        "        return masks.cpu()\n",
        "\n",
        "    def predict(self, image_pil, text_prompt, box_threshold=0.28, text_threshold=0.25):\n",
        "        boxes, logits, phrases = self.predict_dino(image_pil, text_prompt, box_threshold, text_threshold)\n",
        "        masks = torch.tensor([])\n",
        "        if len(boxes) > 0:\n",
        "            masks = self.predict_sam(image_pil, boxes)\n",
        "            masks = masks.squeeze(1)\n",
        "        return masks, boxes, phrases, logits"
      ],
      "metadata": {
        "id": "RXrU4g7BMOu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "box_threshold: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.\n",
        "\n",
        "text_threshold: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.\n",
        "\n",
        "The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts.\n"
      ],
      "metadata": {
        "id": "Zrstnu_fARcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "index = '2_37'\n",
        "\n",
        "image = f'/content/{index}_img.png'\n",
        "ground_truth_masks = f'/content/{index}_gt.png'\n",
        "\n",
        "model = LangSAM()\n",
        "\n",
        "image_pil = Image.open(image).convert(\"RGB\")\n",
        "text_prompt = \"house\"\n",
        "masks, boxes, phrases, logits = model.predict(image_pil, text_prompt)\n",
        "ground_truth_masks = load_ground_truth_masks(ground_truth_masks)\n",
        "\n",
        "#This function displays the original image , predicted masks , and accuracy compared to ground truth\n",
        "display_images_with_masks(image_pil, masks)"
      ],
      "metadata": {
        "id": "y2e5pW-SAPHD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}